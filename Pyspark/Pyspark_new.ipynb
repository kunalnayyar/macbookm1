{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af876e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.4.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0c1277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b9b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col,struct,when,explode\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StringType,IntegerType,FloatType,StructType,StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ff6729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/18 20:46:24 WARN Utils: Your hostname, Kunals-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.68.50 instead (on interface en0)\n",
      "23/06/18 20:46:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/18 20:46:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"pysparktest\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1454d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.68.50:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pysparktest</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x114045390>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4634a477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = spark._sc.parallelize([1,2,3])\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e254122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,\"AZ-201\",23.2),(2,\"DP-104\",112.2),(3,\"DP-203\",99.89)]\n",
    "rdd1 = spark._sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dbf3ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'AZ-201', 23.2), (2, 'DP-104', 112.2), (3, 'DP-203', 99.89)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9cbeca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = rdd1.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c34b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| _1|    _2|   _3|\n",
      "+---+------+-----+\n",
      "|  1|AZ-201| 23.2|\n",
      "|  2|DP-104|112.2|\n",
      "|  3|DP-203|99.89|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1c00800",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"ID\",IntegerType(),True),StructField(\"Course\",StringType(),True),StructField(\"Price\",FloatType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cb59e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| ID|Course|Price|\n",
      "+---+------+-----+\n",
      "|  1|AZ-201| 23.2|\n",
      "|  2|DP-104|112.2|\n",
      "|  3|DP-203|99.89|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(data = data,schema= schema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbf4cac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Course: string (nullable = true)\n",
      " |-- Price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "709e5952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID', 'Course', 'Price']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80bc0b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "581b079c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| ID|Course|Price|\n",
      "+---+------+-----+\n",
      "|  2|DP-104|112.2|\n",
      "|  3|DP-203|99.89|\n",
      "|  1|AZ-201| 23.2|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.sort(F.col(\"Price\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bff437d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| ID|Course|Price|\n",
      "+---+------+-----+\n",
      "|  2|DP-104|112.2|\n",
      "+---+------+-----+\n",
      "\n",
      "+---+------+-----+\n",
      "| ID|Course|Price|\n",
      "+---+------+-----+\n",
      "|  2|DP-104|112.2|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.where(\"Course == 'DP-104'\").show()\n",
    "df1.where(F.col(\"Course\")==\"DP-104\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39387464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 35:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Price)|\n",
      "+-----------------+\n",
      "|78.42999903361003|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.agg(F.avg(F.col(\"Price\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d30531c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'Pyspark_new.ipynb',\n",
       " 'restaurant_orders.csv',\n",
       " 'spark-warehouse',\n",
       " '.ipynb_checkpoints',\n",
       " 'Pyspark_Python.ipynb',\n",
       " 'data']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "42cd199d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Log.parquet', 'customer_obj.json', 'customer_arr.json']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8b30ce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------+------------+----------+\n",
      "|courses                 |customerid|customername|registered|\n",
      "+------------------------+----------+------------+----------+\n",
      "|[AZ-900, AZ-500, AZ-303]|1         |UserA       |true      |\n",
      "|[AZ-104, AZ-500, DP-200]|2         |UserB       |true      |\n",
      "+------------------------+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsondf = spark.read.options(header = \"true\",inferSchema = \"true\").format(\"json\").load(\"data/customer_arr.json\")\n",
    "jsondf.show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4a2c0de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 123:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+---------+--------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| Id|       Correlationid|       Operationname|   Status| Eventcategory|        Level|                Time|        Subscription|    Eventinitiatedby|        Resourcetype|       Resourcegroup|\n",
      "+---+--------------------+--------------------+---------+--------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  1|66641e13-d19f-4ce...| Delete SQL database|Succeeded|Administrative|Informational|2021-06-15 10:14:...|20c6eec9-2d80-470...|Microsoft Azure S...|Microsoft.Sql/ser...|synapseworkspace-...|\n",
      "|  2|66641e13-d19f-4ce...| Delete SQL database|  Started|Administrative|Informational|2021-06-15 10:14:...|20c6eec9-2d80-470...|Microsoft Azure S...|Microsoft.Sql/ser...|synapseworkspace-...|\n",
      "|  3|66641e13-d19f-4ce...| Delete SQL database| Accepted|Administrative|Informational|2021-06-15 10:14:...|20c6eec9-2d80-470...|Microsoft Azure S...|Microsoft.Sql/ser...|synapseworkspace-...|\n",
      "|  4|e2958162-93d9-464...|     Delete SqlPools|Succeeded|Administrative|Informational|2021-06-15 10:14:...|20c6eec9-2d80-470...|techsup1000@gmail...|Microsoft.Synapse...|             new-grp|\n",
      "|  5|e2958162-93d9-464...|     Delete SqlPools|  Started|Administrative|Informational|2021-06-15 10:14:...|20c6eec9-2d80-470...|techsup1000@gmail...|Microsoft.Synapse...|             new-grp|\n",
      "|  6|e2958162-93d9-464...|     Delete SqlPools| Accepted|Administrative|Informational|2021-06-15 10:14:...|20c6eec9-2d80-470...|techsup1000@gmail...|Microsoft.Synapse...|             new-grp|\n",
      "|  7|08cd2e19-477c-4ec...|Pause SQL Analyti...|Succeeded|Administrative|Informational|2021-06-14 23:27:...|20c6eec9-2d80-470...|techsup1000@gmail...|Microsoft.Synapse...|             new-grp|\n",
      "|  8|08cd2e19-477c-4ec...|Pause SQL Analyti...|  Started|Administrative|Informational|2021-06-14 23:25:...|20c6eec9-2d80-470...|techsup1000@gmail...|Microsoft.Synapse...|             new-grp|\n",
      "|  9|08cd2e19-477c-4ec...|Pause SQL Analyti...| Accepted|Administrative|Informational|2021-06-14 23:25:...|20c6eec9-2d80-470...|techsup1000@gmail...|Microsoft.Synapse...|             new-grp|\n",
      "| 10|d2d9d7c4-2766-4e7...|Pause a Datawareh...|Succeeded|Administrative|Informational|2021-06-14 23:26:...|20c6eec9-2d80-470...|Microsoft Azure S...|Microsoft.Sql/ser...|synapseworkspace-...|\n",
      "+---+--------------------+--------------------+---------+--------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "logdf = spark.read.options(header=\"true\",inferSchema = \"true\").format(\"parquet\").load(\"data/Log.parquet\")\n",
    "logdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4f369a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Correlationid: string (nullable = true)\n",
      " |-- Operationname: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Eventcategory: string (nullable = true)\n",
      " |-- Level: string (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Subscription: string (nullable = true)\n",
      " |-- Eventinitiatedby: string (nullable = true)\n",
      " |-- Resourcetype: string (nullable = true)\n",
      " |-- Resourcegroup: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fae497c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "|Order Number|      Order Date|           Item Name|Quantity|Product Price|Total products|\n",
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "|       16118|03/08/2019 20:25|       Plain Papadum|       2|          0.8|             6|\n",
      "|       16118|03/08/2019 20:25|    King Prawn Balti|       1|        12.95|             6|\n",
      "|       16118|03/08/2019 20:25|         Garlic Naan|       1|         2.95|             6|\n",
      "|       16118|03/08/2019 20:25|       Mushroom Rice|       1|         3.95|             6|\n",
      "|       16118|03/08/2019 20:25| Paneer Tikka Masala|       1|         8.95|             6|\n",
      "|       16118|03/08/2019 20:25|       Mango Chutney|       1|          0.5|             6|\n",
      "|       16117|03/08/2019 20:17|          Plain Naan|       1|          2.6|             7|\n",
      "|       16117|03/08/2019 20:17|       Mushroom Rice|       1|         3.95|             7|\n",
      "|       16117|03/08/2019 20:17|Tandoori Chicken ...|       1|         4.95|             7|\n",
      "|       16117|03/08/2019 20:17|     Vindaloo - Lamb|       1|         7.95|             7|\n",
      "|       16117|03/08/2019 20:17|             Chapati|       1|         1.95|             7|\n",
      "|       16117|03/08/2019 20:17|          Lamb Tikka|       1|         4.95|             7|\n",
      "|       16117|03/08/2019 20:17|         Saag Paneer|       1|         5.95|             7|\n",
      "|       16116|03/08/2019 20:09|          Aloo Chaat|       1|         4.95|             5|\n",
      "|       16116|03/08/2019 20:09|      Chicken Pakora|       1|         5.95|             5|\n",
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.option(\"header\",\"true\").format(\"csv\").load(\"restaurant_orders.csv\")\n",
    "df2.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f820fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "|Order Number|      Order Date|           Item Name|Quantity|Product Price|Total products|\n",
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "|       16118|03/08/2019 20:25|       Plain Papadum|       2|          0.8|             6|\n",
      "|       16118|03/08/2019 20:25|    King Prawn Balti|       1|        12.95|             6|\n",
      "|       16118|03/08/2019 20:25|         Garlic Naan|       1|         2.95|             6|\n",
      "|       16118|03/08/2019 20:25|       Mushroom Rice|       1|         3.95|             6|\n",
      "|       16118|03/08/2019 20:25| Paneer Tikka Masala|       1|         8.95|             6|\n",
      "|       16118|03/08/2019 20:25|       Mango Chutney|       1|          0.5|             6|\n",
      "|       16117|03/08/2019 20:17|          Plain Naan|       1|          2.6|             7|\n",
      "|       16117|03/08/2019 20:17|       Mushroom Rice|       1|         3.95|             7|\n",
      "|       16117|03/08/2019 20:17|Tandoori Chicken ...|       1|         4.95|             7|\n",
      "|       16117|03/08/2019 20:17|     Vindaloo - Lamb|       1|         7.95|             7|\n",
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.options( header = \"true\",inferSchema=\"true\").csv(\"restaurant_orders.csv\")\n",
    "df3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9fe8663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order Number: integer (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Item Name: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Product Price: double (nullable = true)\n",
      " |-- Total products: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ec74662",
   "metadata": {},
   "source": [
    "#### Without using inferschema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02fc5e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order Number: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Item Name: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Product Price: string (nullable = true)\n",
      " |-- Total products: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "350f706f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74818"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b5d3e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/16 21:29:22 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------------+--------------+------------------+-----------------+------------------+\n",
      "|summary|      Order Number|      Order Date|     Item Name|          Quantity|    Product Price|    Total products|\n",
      "+-------+------------------+----------------+--------------+------------------+-----------------+------------------+\n",
      "|  count|             74818|           74818|         74818|             74818|            74818|             74818|\n",
      "|   mean|  9115.63816193964|            null|          null|  1.24356438290251|5.286491886982787|  6.93143361223235|\n",
      "| stddev|4052.2104520331745|            null|          null|0.7982073410496792|  3.3382213559897|3.9548324912473527|\n",
      "|    min|             10000|01/01/2017 17:31|    Aloo Chaat|                 1|              0.5|                 1|\n",
      "|    25%|            5590.0|            null|          null|               1.0|             2.95|               5.0|\n",
      "|    50%|            9102.0|            null|          null|               1.0|             3.95|               6.0|\n",
      "|    75%|           12630.0|            null|          null|               1.0|             8.95|               8.0|\n",
      "|    max|              9999|31/12/2018 21:56|Vindaloo Sauce|                 9|             9.95|                 9|\n",
      "+-------+------------------+----------------+--------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6483d7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Order Number',\n",
       " 'Order Date',\n",
       " 'Item Name',\n",
       " 'Quantity',\n",
       " 'Product Price',\n",
       " 'Total products']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "81be638d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"fields\":[{\"metadata\":{},\"name\":\"Order Number\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Order Date\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Item Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Quantity\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Product Price\",\"nullable\":true,\"type\":\"double\"},{\"metadata\":{},\"name\":\"Total products\",\"nullable\":true,\"type\":\"integer\"}],\"type\":\"struct\"}'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.schema.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "180eb468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product Price: string (nullable = true)\n",
      " |-- Total products: string (nullable = true)\n",
      " |-- NewInfo: struct (nullable = false)\n",
      " |    |-- Order_num: string (nullable = true)\n",
      " |    |-- Cost: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf2 = df2.withColumn(\"NewInfo\",struct(col(\"Order Number\").alias(\"Order_num\"),when(col(\"Product Price\")>5.0,\"Cheap\").otherwise(\"Expensive\").alias(\"Cost\"))).drop('Order Number',\n",
    " 'Order Date',\n",
    " 'Item Name',\n",
    " 'Quantity')\n",
    "newdf2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "deede220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+--------------------+-------------+------------+\n",
      "| Id|       Operationname|   Status|                Time|Resourcegroup|ResourceType|\n",
      "+---+--------------------+---------+--------------------+-------------+------------+\n",
      "|195|Check Server Name...|Succeeded|2021-06-14 18:14:...|         null|        null|\n",
      "|196|Check Server Name...|  Started|2021-06-14 18:14:...|         null|        null|\n",
      "|197|Check Server Name...|Succeeded|2021-06-14 18:13:...|         null|        null|\n",
      "|198|Check Server Name...|  Started|2021-06-14 18:13:...|         null|        null|\n",
      "|199|Check Server Name...|Succeeded|2021-06-14 18:13:...|         null|        null|\n",
      "+---+--------------------+---------+--------------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logdf.filter(col(\"Resourcegroup\").isNull()).select(\"Id\",\"Operationname\",\"Status\",\"Time\",\"Resourcegroup\",\"ResourceType\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "17166f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+--------------------+--------------------+--------------------+\n",
      "| Id|      Operationname|   Status|                Time|       Resourcegroup|        ResourceType|\n",
      "+---+-------------------+---------+--------------------+--------------------+--------------------+\n",
      "|  1|Delete SQL database|Succeeded|2021-06-15 10:14:...|synapseworkspace-...|Microsoft.Sql/ser...|\n",
      "|  2|Delete SQL database|  Started|2021-06-15 10:14:...|synapseworkspace-...|Microsoft.Sql/ser...|\n",
      "|  3|Delete SQL database| Accepted|2021-06-15 10:14:...|synapseworkspace-...|Microsoft.Sql/ser...|\n",
      "|  4|    Delete SqlPools|Succeeded|2021-06-15 10:14:...|             new-grp|Microsoft.Synapse...|\n",
      "|  5|    Delete SqlPools|  Started|2021-06-15 10:14:...|             new-grp|Microsoft.Synapse...|\n",
      "+---+-------------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logdf.filter(col(\"Resourcegroup\").isNotNull()).select(\"Id\",\"Operationname\",\"Status\",\"Time\",\"Resourcegroup\",\"ResourceType\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7e681307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|           Item Name|Total_count|\n",
      "+--------------------+-----------+\n",
      "|          Pilau Rice|       4721|\n",
      "|          Plain Naan|       3753|\n",
      "|       Plain Papadum|       3598|\n",
      "|         Garlic Naan|       2628|\n",
      "|        Onion Bhajee|       2402|\n",
      "|          Plain Rice|       2369|\n",
      "|Chicken Tikka Masala|       2133|\n",
      "|       Mango Chutney|       2070|\n",
      "|         Bombay Aloo|       1752|\n",
      "|       Peshwari Naan|       1535|\n",
      "|          Mint Sauce|       1463|\n",
      "|       Mushroom Rice|       1452|\n",
      "|          Keema Naan|       1362|\n",
      "|               Korma|       1201|\n",
      "|           Saag Aloo|       1194|\n",
      "|         Meat Samosa|       1192|\n",
      "|             Chapati|       1170|\n",
      "|       Onion Chutney|       1033|\n",
      "|      Butter Chicken|        980|\n",
      "|     Korma - Chicken|        943|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy(col(\"Item Name\")).count().select(col(\"Item Name\"),col(\"count\").alias(\"Total_count\")).sort(col(\"Total_count\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bf83eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "curr = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8bd49397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------+---------+-------------+-----------------------+\n",
      "|ss                       |year(ss)|month(ss)|dayofyear(ss)|to_date(ss, dd/mm/yyyy)|\n",
      "+-------------------------+--------+---------+-------------+-----------------------+\n",
      "|2023-04-16 23:32:52.58307|2023    |4        |106          |2023-04-16             |\n",
      "|2023-04-16 23:32:52.58307|2023    |4        |106          |2023-04-16             |\n",
      "|2023-04-16 23:32:52.58307|2023    |4        |106          |2023-04-16             |\n",
      "+-------------------------+--------+---------+-------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"ss\",lit(curr)).select(col(\"ss\"),year(col(\"ss\")),month(col(\"ss\")),dayofyear(col(\"ss\")),to_date(col(\"ss\"),\"dd/mm/yyyy\")).show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4f55c5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+----------+\n",
      "|             courses|customerid|customername|registered|\n",
      "+--------------------+----------+------------+----------+\n",
      "|[AZ-900, AZ-500, ...|         1|       UserA|      true|\n",
      "|[AZ-104, AZ-500, ...|         2|       UserB|      true|\n",
      "+--------------------+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsondf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "23173e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+\n",
      "|Courses|customerid|customername|registered|\n",
      "+-------+----------+------------+----------+\n",
      "| AZ-900|         1|       UserA|      true|\n",
      "| AZ-500|         1|       UserA|      true|\n",
      "| AZ-303|         1|       UserA|      true|\n",
      "| AZ-104|         2|       UserB|      true|\n",
      "| AZ-500|         2|       UserB|      true|\n",
      "| DP-200|         2|       UserB|      true|\n",
      "+-------+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsondf.select(explode(col(\"courses\")).alias(\"Courses\"),col(\"customerid\"),col(\"customername\"),col(\"registered\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4ea87cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Log.parquet', 'customer_obj.json', 'customer_arr.json']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e261ce94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------+------------+-----------------+----------+\n",
      "|courses                 |customerid|customername|details          |registered|\n",
      "+------------------------+----------+------------+-----------------+----------+\n",
      "|[AZ-900, AZ-500, AZ-303]|1         |UserA       |{CityA, 111-1112}|true      |\n",
      "|[AZ-104, AZ-500, DP-200]|2         |UserB       |{CityB, 333-1112}|true      |\n",
      "+------------------------+----------+------------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custdf = spark.read.options(header= \"true\",inferschema = \"true\").format(\"json\").load('data/customer_obj.json')\n",
    "custdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b49a905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- customerid: long (nullable = true)\n",
      " |-- customername: string (nullable = true)\n",
      " |-- details: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |-- registered: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17d7c39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| city|  mobile|\n",
      "+-----+--------+\n",
      "|CityA|111-1112|\n",
      "|CityB|333-1112|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custdf.select(col(\"details.*\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de2e6c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+--------------+------+\n",
      "|customername| city|details.mobile|   col|\n",
      "+------------+-----+--------------+------+\n",
      "|       UserA|CityA|      111-1112|AZ-900|\n",
      "|       UserA|CityA|      111-1112|AZ-500|\n",
      "|       UserA|CityA|      111-1112|AZ-303|\n",
      "|       UserB|CityB|      333-1112|AZ-104|\n",
      "|       UserB|CityB|      333-1112|AZ-500|\n",
      "|       UserB|CityB|      333-1112|DP-200|\n",
      "+------------+-----+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custdf.select(col('customername'),col('details.city'),col('details').mobile,explode(col(\"courses\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3abe4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema2 = StructType().add(\"col1\",StringType()).add(\"col2\",IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a617e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"fields\":[{\"metadata\":{},\"name\":\"col1\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"col2\",\"nullable\":true,\"type\":\"integer\"}],\"type\":\"struct\"}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema2.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8fcab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
