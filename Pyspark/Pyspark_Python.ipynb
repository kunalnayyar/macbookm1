{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870f347d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.3.2.tar.gz (281.4 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824010 sha256=3aa3c429ea831347625acced2cc399d94dda2f7c532b5877646292a6a824b93b\n",
      "  Stored in directory: /Users/koffeebook/Library/Caches/pip/wheels/47/69/84/c7c7776e2287a654536f5cba7dc54c904c03aa2c3e29206f0f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a21449d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.0-cp311-cp311-macosx_10_9_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/koffeebook/opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.21.0\n",
      "  Downloading numpy-1.24.2-cp311-cp311-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/koffeebook/opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.24.2 pandas-2.0.0 pytz-2023.3 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d86e4b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe854de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 18:41:52 WARN Utils: Your hostname, Kunals-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.68.54 instead (on interface en0)\n",
      "23/04/16 18:41:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 18:41:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext(\"local\",\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d91fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a411f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da44c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pysparktest\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee6a6ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.68.54:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x114acdd90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c41a24f",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "298b58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b57d17cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['restaurant_orders.csv',\n",
       " 'spark-warehouse',\n",
       " '.ipynb_checkpoints',\n",
       " 'Pyspark_Python.ipynb']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "93b8aeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order Number</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Item Name</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Product Price</th>\n",
       "      <th>Total products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16118</td>\n",
       "      <td>03/08/2019 20:25</td>\n",
       "      <td>Plain Papadum</td>\n",
       "      <td>2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16118</td>\n",
       "      <td>03/08/2019 20:25</td>\n",
       "      <td>King Prawn Balti</td>\n",
       "      <td>1</td>\n",
       "      <td>12.95</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16118</td>\n",
       "      <td>03/08/2019 20:25</td>\n",
       "      <td>Garlic Naan</td>\n",
       "      <td>1</td>\n",
       "      <td>2.95</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16118</td>\n",
       "      <td>03/08/2019 20:25</td>\n",
       "      <td>Mushroom Rice</td>\n",
       "      <td>1</td>\n",
       "      <td>3.95</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16118</td>\n",
       "      <td>03/08/2019 20:25</td>\n",
       "      <td>Paneer Tikka Masala</td>\n",
       "      <td>1</td>\n",
       "      <td>8.95</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74813</th>\n",
       "      <td>2518</td>\n",
       "      <td>18/05/2016 18:53</td>\n",
       "      <td>House Red wine 75cl</td>\n",
       "      <td>2</td>\n",
       "      <td>17.95</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74814</th>\n",
       "      <td>2503</td>\n",
       "      <td>14/05/2016 19:28</td>\n",
       "      <td>House Red wine 75cl</td>\n",
       "      <td>2</td>\n",
       "      <td>17.95</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74815</th>\n",
       "      <td>2467</td>\n",
       "      <td>10/05/2016 13:18</td>\n",
       "      <td>House Red wine 75cl</td>\n",
       "      <td>2</td>\n",
       "      <td>17.95</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74816</th>\n",
       "      <td>2249</td>\n",
       "      <td>07/04/2016 19:04</td>\n",
       "      <td>House white wine 75cl</td>\n",
       "      <td>1</td>\n",
       "      <td>17.95</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74817</th>\n",
       "      <td>1403</td>\n",
       "      <td>02/10/2015 17:29</td>\n",
       "      <td>House Red wine 75cl</td>\n",
       "      <td>1</td>\n",
       "      <td>17.95</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74818 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Order Number        Order Date              Item Name  Quantity   \n",
       "0             16118  03/08/2019 20:25          Plain Papadum         2  \\\n",
       "1             16118  03/08/2019 20:25       King Prawn Balti         1   \n",
       "2             16118  03/08/2019 20:25            Garlic Naan         1   \n",
       "3             16118  03/08/2019 20:25          Mushroom Rice         1   \n",
       "4             16118  03/08/2019 20:25    Paneer Tikka Masala         1   \n",
       "...             ...               ...                    ...       ...   \n",
       "74813          2518  18/05/2016 18:53    House Red wine 75cl         2   \n",
       "74814          2503  14/05/2016 19:28    House Red wine 75cl         2   \n",
       "74815          2467  10/05/2016 13:18    House Red wine 75cl         2   \n",
       "74816          2249  07/04/2016 19:04  House white wine 75cl         1   \n",
       "74817          1403  02/10/2015 17:29    House Red wine 75cl         1   \n",
       "\n",
       "       Product Price  Total products  \n",
       "0               0.80               6  \n",
       "1              12.95               6  \n",
       "2               2.95               6  \n",
       "3               3.95               6  \n",
       "4               8.95               6  \n",
       "...              ...             ...  \n",
       "74813          17.95               7  \n",
       "74814          17.95               6  \n",
       "74815          17.95               9  \n",
       "74816          17.95               6  \n",
       "74817          17.95               9  \n",
       "\n",
       "[74818 rows x 6 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"restaurant_orders.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b6a33a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>marks</th>\n",
       "      <th>marks2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f</td>\n",
       "      <td>46.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name  marks  marks2\n",
       "0    a   34.0    34.0\n",
       "1    b   50.0    20.0\n",
       "2    c   45.0     NaN\n",
       "3    d   12.0    12.0\n",
       "4    e   50.0     NaN\n",
       "5    f   46.0    18.0\n",
       "6    h    NaN    22.0\n",
       "7    i    NaN    21.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df1,df2,how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f80113a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res(marks):\n",
    "    if marks>30:\n",
    "        return \"Pass\"\n",
    "    else:\n",
    "        return \"Fail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a94cc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>marks2</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>34</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>50</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h</td>\n",
       "      <td>22</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>12</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>12</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i</td>\n",
       "      <td>34</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name  marks2 result\n",
       "0    a      34   Pass\n",
       "1    b      50   Pass\n",
       "2    h      22   Fail\n",
       "3    d      12   Fail\n",
       "4    e      12   Fail\n",
       "5    i      34   Pass"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['result'] = df['marks2'].apply(lambda x:res(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae6a399",
   "metadata": {},
   "source": [
    "### Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a457fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",\"true\").csv(\"restaurant_orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d9d62e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "|Order Number|      Order Date|           Item Name|Quantity|Product Price|Total products|\n",
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "|       16118|03/08/2019 20:25|       Plain Papadum|       2|          0.8|             6|\n",
      "|       16118|03/08/2019 20:25|    King Prawn Balti|       1|        12.95|             6|\n",
      "|       16118|03/08/2019 20:25|         Garlic Naan|       1|         2.95|             6|\n",
      "|       16118|03/08/2019 20:25|       Mushroom Rice|       1|         3.95|             6|\n",
      "|       16118|03/08/2019 20:25| Paneer Tikka Masala|       1|         8.95|             6|\n",
      "|       16118|03/08/2019 20:25|       Mango Chutney|       1|          0.5|             6|\n",
      "|       16117|03/08/2019 20:17|          Plain Naan|       1|          2.6|             7|\n",
      "|       16117|03/08/2019 20:17|       Mushroom Rice|       1|         3.95|             7|\n",
      "|       16117|03/08/2019 20:17|Tandoori Chicken ...|       1|         4.95|             7|\n",
      "|       16117|03/08/2019 20:17|     Vindaloo - Lamb|       1|         7.95|             7|\n",
      "|       16117|03/08/2019 20:17|             Chapati|       1|         1.95|             7|\n",
      "|       16117|03/08/2019 20:17|          Lamb Tikka|       1|         4.95|             7|\n",
      "|       16117|03/08/2019 20:17|         Saag Paneer|       1|         5.95|             7|\n",
      "|       16116|03/08/2019 20:09|          Aloo Chaat|       1|         4.95|             5|\n",
      "|       16116|03/08/2019 20:09|      Chicken Pakora|       1|         5.95|             5|\n",
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdbf9af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Order Number',\n",
       " 'Order Date',\n",
       " 'Item Name',\n",
       " 'Quantity',\n",
       " 'Product Price',\n",
       " 'Total products']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09ac3d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------------+--------------+------------------+-----------------+------------------+\n",
      "|summary|      Order Number|      Order Date|     Item Name|          Quantity|    Product Price|    Total products|\n",
      "+-------+------------------+----------------+--------------+------------------+-----------------+------------------+\n",
      "|  count|             74818|           74818|         74818|             74818|            74818|             74818|\n",
      "|   mean|  9115.63816193964|            null|          null|  1.24356438290251|5.286491886982787|  6.93143361223235|\n",
      "| stddev|4052.2104520331745|            null|          null|0.7982073410496792|  3.3382213559897|3.9548324912473527|\n",
      "|    min|             10000|01/01/2017 17:31|    Aloo Chaat|                 1|              0.5|                 1|\n",
      "|    25%|            5590.0|            null|          null|               1.0|             2.95|               5.0|\n",
      "|    50%|            9102.0|            null|          null|               1.0|             3.95|               6.0|\n",
      "|    75%|           12630.0|            null|          null|               1.0|             8.95|               8.0|\n",
      "|    max|              9999|31/12/2018 21:56|Vindaloo Sauce|                 9|             9.95|                 9|\n",
      "+-------+------------------+----------------+--------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5efc262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74818"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7225551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------+--------+\n",
      "|           Item Name|total_orders|Product Price| revenue|\n",
      "+--------------------+------------+-------------+--------+\n",
      "|Chicken Tikka Masala|        2473|         8.95|22133.35|\n",
      "|          Pilau Rice|        6367|         2.95|18782.65|\n",
      "|               Korma|        1370|        12.95| 17741.5|\n",
      "|               Korma|        1370|         9.95| 13631.5|\n",
      "|          Plain Naan|        4983|          2.6| 12955.8|\n",
      "|               Korma|        1370|         8.95| 12261.5|\n",
      "|         Bombay Aloo|        1831|         5.95|10894.45|\n",
      "|        Onion Bhajee|        2749|         3.95|10858.55|\n",
      "|      Butter Chicken|        1068|         9.95| 10626.6|\n",
      "|              Madras|         851|        11.95|10169.45|\n",
      "|         Garlic Naan|        3318|         2.95|  9788.1|\n",
      "|     Korma - Chicken|        1091|         8.95| 9764.45|\n",
      "|          Plain Rice|        2964|         2.95|  8743.8|\n",
      "|Tandoori Mixed Grill|         716|        11.95|  8556.2|\n",
      "|       Plain Papadum|       10648|          0.8|  8518.4|\n",
      "|Chicken Tikka (Main)|         905|         8.95| 8099.75|\n",
      "|     Chicken Biryani|         771|         9.95| 7671.45|\n",
      "|               Curry|         639|        11.95| 7636.05|\n",
      "|              Madras|         851|         8.95| 7616.45|\n",
      "|           Saag Aloo|        1234|         5.95|  7342.3|\n",
      "+--------------------+------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totaldf = df.groupBy(col(\"Item Name\")).agg(sum(col(\"Quantity\").cast(\"int\")).alias(\"total_orders\"))\n",
    "totaldf.join(df.select(col(\"Item Name\"),col(\"Product Price\")).distinct(),\"Item Name\").withColumn(\"revenue\",round(col(\"total_orders\")*col(\"Product Price\"),2)).orderBy(col(\"revenue\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ec998f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"Orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92c0de08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "|Order Number|      Order Date|           Item Name|Quantity|Product Price|Total products|\n",
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "|       16118|03/08/2019 20:25|       Plain Papadum|       2|          0.8|             6|\n",
      "|       16118|03/08/2019 20:25|    King Prawn Balti|       1|        12.95|             6|\n",
      "|       16118|03/08/2019 20:25|         Garlic Naan|       1|         2.95|             6|\n",
      "|       16118|03/08/2019 20:25|       Mushroom Rice|       1|         3.95|             6|\n",
      "|       16118|03/08/2019 20:25| Paneer Tikka Masala|       1|         8.95|             6|\n",
      "|       16118|03/08/2019 20:25|       Mango Chutney|       1|          0.5|             6|\n",
      "|       16117|03/08/2019 20:17|          Plain Naan|       1|          2.6|             7|\n",
      "|       16117|03/08/2019 20:17|       Mushroom Rice|       1|         3.95|             7|\n",
      "|       16117|03/08/2019 20:17|Tandoori Chicken ...|       1|         4.95|             7|\n",
      "|       16117|03/08/2019 20:17|     Vindaloo - Lamb|       1|         7.95|             7|\n",
      "|       16117|03/08/2019 20:17|             Chapati|       1|         1.95|             7|\n",
      "|       16117|03/08/2019 20:17|          Lamb Tikka|       1|         4.95|             7|\n",
      "|       16117|03/08/2019 20:17|         Saag Paneer|       1|         5.95|             7|\n",
      "|       16116|03/08/2019 20:09|          Aloo Chaat|       1|         4.95|             5|\n",
      "|       16116|03/08/2019 20:09|      Chicken Pakora|       1|         5.95|             5|\n",
      "|       16116|03/08/2019 20:09|Chicken Chilli Ga...|       1|         9.95|             5|\n",
      "|       16116|03/08/2019 20:09|        Lamb Biryani|       1|         9.95|             5|\n",
      "|       16116|03/08/2019 20:09|          Keema Naan|       1|         2.95|             5|\n",
      "|       16115|03/08/2019 20:01|      Chicken Pakora|       1|         5.95|             7|\n",
      "|       16115|03/08/2019 20:01|       Plain Papadum|       1|          0.8|             7|\n",
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from Orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e44802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------+\n",
      "|      col_name|data_type|comment|\n",
      "+--------------+---------+-------+\n",
      "|  Order Number|   string|   null|\n",
      "|    Order Date|   string|   null|\n",
      "|     Item Name|   string|   null|\n",
      "|      Quantity|   string|   null|\n",
      "| Product Price|   string|   null|\n",
      "|Total products|   string|   null|\n",
      "+--------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe Orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5e3e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dafb957f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90682a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/15 23:31:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/15 23:31:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/15 23:31:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/15 23:31:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/15 23:31:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/15 23:31:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/15 23:31:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+--------------------+---+\n",
      "|           Item Name| rn|\n",
      "+--------------------+---+\n",
      "|       Plain Papadum|  1|\n",
      "|          Pilau Rice|  2|\n",
      "|          Plain Naan|  3|\n",
      "|         Garlic Naan|  4|\n",
      "|          Plain Rice|  5|\n",
      "|        Onion Bhajee|  6|\n",
      "|       Mango Chutney|  7|\n",
      "|Chicken Tikka Masala|  8|\n",
      "|             Chapati|  9|\n",
      "|          Mint Sauce| 10|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from (select `Item Name`,row_number() over(order by total_orders desc) as `rn` from (select `Item Name`,sum(cast(Quantity as int)) as total_orders from Orders as total_orders group by `Item Name`) )as temp where `rn` <=10 \").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7a80449",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'shuffle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mshuffle\u001b[38;5;241m.\u001b[39mpartitions\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'shuffle'"
     ]
    }
   ],
   "source": [
    "spark.sql.shuffle.partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80fed59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Builder',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activeSession',\n",
       " '_conf',\n",
       " '_convert_from_pandas',\n",
       " '_createFromLocal',\n",
       " '_createFromRDD',\n",
       " '_create_dataframe',\n",
       " '_create_from_pandas_with_arrow',\n",
       " '_create_shell_session',\n",
       " '_getActiveSessionOrCreate',\n",
       " '_get_numpy_record_dtype',\n",
       " '_inferSchema',\n",
       " '_inferSchemaFromList',\n",
       " '_instantiatedSession',\n",
       " '_jconf',\n",
       " '_jsc',\n",
       " '_jsparkSession',\n",
       " '_jvm',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " 'builder',\n",
       " 'catalog',\n",
       " 'conf',\n",
       " 'createDataFrame',\n",
       " 'getActiveSession',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'sparkContext',\n",
       " 'sql',\n",
       " 'stop',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'udf',\n",
       " 'version']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "092b6efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = spark._sc.parallelize([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b37c246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44c63bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3016e1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/koffeebook/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/koffeebook/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/Users/koffeebook/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/koffeebook/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/koffeebook/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/koffeebook/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/koffeebook/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/koffeebook/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/koffeebook/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in <dictcomp>\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/serializers.py:458\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cloudpickle\u001b[38;5;241m.\u001b[39mdumps(obj, pickle_protocol)\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m cp\u001b[38;5;241m.\u001b[39mdump(obj)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:602\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mself\u001b[39m, obj)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:692\u001b[0m, in \u001b[0;36mCloudPickler.reducer_override\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_reduce(obj)\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:565\u001b[0m, in \u001b[0;36mCloudPickler._function_reduce\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynamic_function_reduce(obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:546\u001b[0m, in \u001b[0;36mCloudPickler._dynamic_function_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    545\u001b[0m newargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_getnewargs(func)\n\u001b[0;32m--> 546\u001b[0m state \u001b[38;5;241m=\u001b[39m _function_getstate(func)\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (types\u001b[38;5;241m.\u001b[39mFunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    548\u001b[0m         _function_setstate)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:157\u001b[0m, in \u001b[0;36m_function_getstate\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    146\u001b[0m slotstate \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__qualname__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__closure__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__closure__\u001b[39m,\n\u001b[1;32m    155\u001b[0m }\n\u001b[0;32m--> 157\u001b[0m f_globals_ref \u001b[38;5;241m=\u001b[39m _extract_code_globals(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m)\n\u001b[1;32m    158\u001b[0m f_globals \u001b[38;5;241m=\u001b[39m {k: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    159\u001b[0m              func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py:334\u001b[0m, in \u001b[0;36m_extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m out_names \u001b[38;5;241m=\u001b[39m {names[oparg]: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py:334\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m out_names \u001b[38;5;241m=\u001b[39m {names[oparg]: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df3 \u001b[38;5;241m=\u001b[39m rdd3\u001b[38;5;241m.\u001b[39mtoDF()\n\u001b[1;32m      2\u001b[0m df3\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/sql/session.py:102\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=1)]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sparkSession\u001b[38;5;241m.\u001b[39mcreateDataFrame(\u001b[38;5;28mself\u001b[39m, schema, sampleRatio)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/sql/session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    896\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/sql/session.py:934\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m--> 934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/sql/session.py:600\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 600\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchema(rdd, samplingRatio, names\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m    601\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    602\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/sql/session.py:546\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inferSchema\u001b[39m(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    527\u001b[0m     rdd: RDD[Any],\n\u001b[1;32m    528\u001b[0m     samplingRatio: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    529\u001b[0m     names: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    530\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StructType:\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;124;03m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m     first \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mfirst()\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m first:\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first row in RDD is empty, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan not infer schema\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/rdd.py:1903\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1891\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   1893\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   1902\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1903\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   1905\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1880\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1882\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 1883\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mrunJob(\u001b[38;5;28mself\u001b[39m, takeUpToNumLeft, p)\n\u001b[1;32m   1885\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   1886\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1484\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1486\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mrunJob(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd, partitions)\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/rdd.py:3505\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3503\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3505\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m _wrap_function(\n\u001b[1;32m   3506\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd_deserializer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer, profiler\n\u001b[1;32m   3507\u001b[0m )\n\u001b[1;32m   3509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3510\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[1;32m   3511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[1;32m   3512\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/rdd.py:3362\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   3360\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3361\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 3362\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[1;32m   3363\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\n\u001b[1;32m   3365\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m   3366\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3371\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[1;32m   3372\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/rdd.py:3345\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, command: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[1;32m   3343\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   3344\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 3345\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m ser\u001b[38;5;241m.\u001b[39mdumps(command)\n\u001b[1;32m   3346\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   3348\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pyspark/serializers.py:468\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    466\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    467\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 468\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "df3 = rdd3.toDF()\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa14ca66",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'getAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m l1 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mgetConf\u001b[38;5;241m.\u001b[39mgetAll\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(l1)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'getAll'"
     ]
    }
   ],
   "source": [
    "l1 = spark.sparkContext.getConf.getAll\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077644f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
