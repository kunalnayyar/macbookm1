{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af876e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.4.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0c1277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17b9b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col,struct,when\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StringType,IntegerType,FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ff6729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/16 19:29:31 WARN Utils: Your hostname, Kunals-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.68.54 instead (on interface en0)\n",
      "23/04/16 19:29:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/16 19:29:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"pysparktest\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1454d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.68.54:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pysparktest</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10d316d90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14eb5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 8) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = spark._sc.parallelize([1,2,3])\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb0106ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,\"AZ-201\",23.2),(2,\"DP-104\",112.2),(3,\"DP-203\",99.89)]\n",
    "rdd1 = spark._sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af115d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'AZ-201', 23.2), (2, 'DP-104', 112.2), (3, 'DP-203', 99.89)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7f9e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = rdd1.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2b87fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| _1|    _2|   _3|\n",
      "+---+------+-----+\n",
      "|  1|AZ-201| 23.2|\n",
      "|  2|DP-104|112.2|\n",
      "|  3|DP-203|99.89|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1b444c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"ID\",IntegerType(),True),StructField(\"Course\",StringType(),True),StructField(\"Price\",FloatType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0257fdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| ID|Course|Price|\n",
      "+---+------+-----+\n",
      "|  1|AZ-201| 23.2|\n",
      "|  2|DP-104|112.2|\n",
      "|  3|DP-203|99.89|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(data = data,schema= schema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af3c266e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Course: string (nullable = true)\n",
      " |-- Price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f4b1718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID', 'Course', 'Price']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d555460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e43d1c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| ID|Course|Price|\n",
      "+---+------+-----+\n",
      "|  2|DP-104|112.2|\n",
      "|  3|DP-203|99.89|\n",
      "|  1|AZ-201| 23.2|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.sort(F.col(\"Price\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2216d2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| ID|Course|Price|\n",
      "+---+------+-----+\n",
      "|  2|DP-104|112.2|\n",
      "+---+------+-----+\n",
      "\n",
      "+---+------+-----+\n",
      "| ID|Course|Price|\n",
      "+---+------+-----+\n",
      "|  2|DP-104|112.2|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.where(\"Course == 'DP-104'\").show()\n",
    "df1.where(F.col(\"Course\")==\"DP-104\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91623c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 35:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Price)|\n",
      "+-----------------+\n",
      "|78.42999903361003|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.agg(F.avg(F.col(\"Price\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "07a43d05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'Pyspark_new.ipynb',\n",
       " 'restaurant_orders.csv',\n",
       " 'spark-warehouse',\n",
       " '.ipynb_checkpoints',\n",
       " 'Pyspark_Python.ipynb',\n",
       " 'data']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b5b6b457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Log.parquet', 'customer_obj.json', 'customer_arr.json']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9723f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 123:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+---------+--------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| Id|       Correlationid|       Operationname|   Status| Eventcategory|        Level|                Time|        Subscription|    Eventinitiatedby|        Resourcetype|       Resourcegroup|\n",
      "+---+--------------------+--------------------+---------+--------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  1|66641e13-d19f-4ce...| Delete SQL database|Succeeded|Administrative|Informational|2021-06-15 10:14:...|20c6eec9-2d80-470...|Microsoft Azure S...|Microsoft.Sql/ser...|synapseworkspace-...|\n",
      "|  2|66641e13-d19f-4ce...| Delete SQL database|  Started|Administrative|Informational|2021-06-15 10:14:...|20c6eec9-2d80-470...|Microsoft Azure S...|Microsoft.Sql/ser...|synapseworkspace-...|\n",
      "|  3|66641e13-d19f-4ce...| Delete SQL database| Accepted|Administrative|Informational|2021-06-15 10:14:...|20c6eec9-2d80-470...|Microsoft Azure S...|Microsoft.Sql/ser...|synapseworkspace-...|\n",
      "|  4|e2958162-93d9-464...|     Delete SqlPools|Succeeded|Administrative|Informational|2021-06-15 10:14:...|20c6eec9-2d80-470...|techsup1000@gmail...|Microsoft.Synapse...|             new-grp|\n",
      "|  5|e2958162-93d9-464...|     Delete SqlPools|  Started|Administrative|Informational|2021-06-15 10:14:...|20c6eec9-2d80-470...|techsup1000@gmail...|Microsoft.Synapse...|             new-grp|\n",
      "|  6|e2958162-93d9-464...|     Delete SqlPools| Accepted|Administrative|Informational|2021-06-15 10:14:...|20c6eec9-2d80-470...|techsup1000@gmail...|Microsoft.Synapse...|             new-grp|\n",
      "|  7|08cd2e19-477c-4ec...|Pause SQL Analyti...|Succeeded|Administrative|Informational|2021-06-14 23:27:...|20c6eec9-2d80-470...|techsup1000@gmail...|Microsoft.Synapse...|             new-grp|\n",
      "|  8|08cd2e19-477c-4ec...|Pause SQL Analyti...|  Started|Administrative|Informational|2021-06-14 23:25:...|20c6eec9-2d80-470...|techsup1000@gmail...|Microsoft.Synapse...|             new-grp|\n",
      "|  9|08cd2e19-477c-4ec...|Pause SQL Analyti...| Accepted|Administrative|Informational|2021-06-14 23:25:...|20c6eec9-2d80-470...|techsup1000@gmail...|Microsoft.Synapse...|             new-grp|\n",
      "| 10|d2d9d7c4-2766-4e7...|Pause a Datawareh...|Succeeded|Administrative|Informational|2021-06-14 23:26:...|20c6eec9-2d80-470...|Microsoft Azure S...|Microsoft.Sql/ser...|synapseworkspace-...|\n",
      "+---+--------------------+--------------------+---------+--------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "logdf = spark.read.options(header=\"true\",inferSchema = \"true\").format(\"parquet\").load(\"data/Log.parquet\")\n",
    "logdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5ee6e416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Correlationid: string (nullable = true)\n",
      " |-- Operationname: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Eventcategory: string (nullable = true)\n",
      " |-- Level: string (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Subscription: string (nullable = true)\n",
      " |-- Eventinitiatedby: string (nullable = true)\n",
      " |-- Resourcetype: string (nullable = true)\n",
      " |-- Resourcegroup: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6eb45a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "|Order Number|      Order Date|           Item Name|Quantity|Product Price|Total products|\n",
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "|       16118|03/08/2019 20:25|       Plain Papadum|       2|          0.8|             6|\n",
      "|       16118|03/08/2019 20:25|    King Prawn Balti|       1|        12.95|             6|\n",
      "|       16118|03/08/2019 20:25|         Garlic Naan|       1|         2.95|             6|\n",
      "|       16118|03/08/2019 20:25|       Mushroom Rice|       1|         3.95|             6|\n",
      "|       16118|03/08/2019 20:25| Paneer Tikka Masala|       1|         8.95|             6|\n",
      "|       16118|03/08/2019 20:25|       Mango Chutney|       1|          0.5|             6|\n",
      "|       16117|03/08/2019 20:17|          Plain Naan|       1|          2.6|             7|\n",
      "|       16117|03/08/2019 20:17|       Mushroom Rice|       1|         3.95|             7|\n",
      "|       16117|03/08/2019 20:17|Tandoori Chicken ...|       1|         4.95|             7|\n",
      "|       16117|03/08/2019 20:17|     Vindaloo - Lamb|       1|         7.95|             7|\n",
      "|       16117|03/08/2019 20:17|             Chapati|       1|         1.95|             7|\n",
      "|       16117|03/08/2019 20:17|          Lamb Tikka|       1|         4.95|             7|\n",
      "|       16117|03/08/2019 20:17|         Saag Paneer|       1|         5.95|             7|\n",
      "|       16116|03/08/2019 20:09|          Aloo Chaat|       1|         4.95|             5|\n",
      "|       16116|03/08/2019 20:09|      Chicken Pakora|       1|         5.95|             5|\n",
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.option(\"header\",\"true\").format(\"csv\").load(\"restaurant_orders.csv\")\n",
    "df2.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b7877c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "|Order Number|      Order Date|           Item Name|Quantity|Product Price|Total products|\n",
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "|       16118|03/08/2019 20:25|       Plain Papadum|       2|          0.8|             6|\n",
      "|       16118|03/08/2019 20:25|    King Prawn Balti|       1|        12.95|             6|\n",
      "|       16118|03/08/2019 20:25|         Garlic Naan|       1|         2.95|             6|\n",
      "|       16118|03/08/2019 20:25|       Mushroom Rice|       1|         3.95|             6|\n",
      "|       16118|03/08/2019 20:25| Paneer Tikka Masala|       1|         8.95|             6|\n",
      "|       16118|03/08/2019 20:25|       Mango Chutney|       1|          0.5|             6|\n",
      "|       16117|03/08/2019 20:17|          Plain Naan|       1|          2.6|             7|\n",
      "|       16117|03/08/2019 20:17|       Mushroom Rice|       1|         3.95|             7|\n",
      "|       16117|03/08/2019 20:17|Tandoori Chicken ...|       1|         4.95|             7|\n",
      "|       16117|03/08/2019 20:17|     Vindaloo - Lamb|       1|         7.95|             7|\n",
      "+------------+----------------+--------------------+--------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.options( header = \"true\",inferSchema=\"true\").csv(\"restaurant_orders.csv\")\n",
    "df3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d8c0f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order Number: integer (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Item Name: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Product Price: double (nullable = true)\n",
      " |-- Total products: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d6c0f3",
   "metadata": {},
   "source": [
    "#### Without using inferschema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3187b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order Number: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Item Name: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Product Price: string (nullable = true)\n",
      " |-- Total products: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc957733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74818"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5fbcbd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/16 21:29:22 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------------+--------------+------------------+-----------------+------------------+\n",
      "|summary|      Order Number|      Order Date|     Item Name|          Quantity|    Product Price|    Total products|\n",
      "+-------+------------------+----------------+--------------+------------------+-----------------+------------------+\n",
      "|  count|             74818|           74818|         74818|             74818|            74818|             74818|\n",
      "|   mean|  9115.63816193964|            null|          null|  1.24356438290251|5.286491886982787|  6.93143361223235|\n",
      "| stddev|4052.2104520331745|            null|          null|0.7982073410496792|  3.3382213559897|3.9548324912473527|\n",
      "|    min|             10000|01/01/2017 17:31|    Aloo Chaat|                 1|              0.5|                 1|\n",
      "|    25%|            5590.0|            null|          null|               1.0|             2.95|               5.0|\n",
      "|    50%|            9102.0|            null|          null|               1.0|             3.95|               6.0|\n",
      "|    75%|           12630.0|            null|          null|               1.0|             8.95|               8.0|\n",
      "|    max|              9999|31/12/2018 21:56|Vindaloo Sauce|                 9|             9.95|                 9|\n",
      "+-------+------------------+----------------+--------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "29459082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Order Number',\n",
       " 'Order Date',\n",
       " 'Item Name',\n",
       " 'Quantity',\n",
       " 'Product Price',\n",
       " 'Total products']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ec7a9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"fields\":[{\"metadata\":{},\"name\":\"Order Number\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Order Date\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Item Name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"Quantity\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"Product Price\",\"nullable\":true,\"type\":\"double\"},{\"metadata\":{},\"name\":\"Total products\",\"nullable\":true,\"type\":\"integer\"}],\"type\":\"struct\"}'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.schema.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d7f4d6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product Price: string (nullable = true)\n",
      " |-- Total products: string (nullable = true)\n",
      " |-- NewInfo: struct (nullable = false)\n",
      " |    |-- Order_num: string (nullable = true)\n",
      " |    |-- Cost: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf2 = df2.withColumn(\"NewInfo\",struct(col(\"Order Number\").alias(\"Order_num\"),when(col(\"Product Price\")>5.0,\"Cheap\").otherwise(\"Expensive\").alias(\"Cost\"))).drop('Order Number',\n",
    " 'Order Date',\n",
    " 'Item Name',\n",
    " 'Quantity')\n",
    "newdf2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "072de6fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+--------------------+-------------+------------+\n",
      "| Id|       Operationname|   Status|                Time|Resourcegroup|ResourceType|\n",
      "+---+--------------------+---------+--------------------+-------------+------------+\n",
      "|195|Check Server Name...|Succeeded|2021-06-14 18:14:...|         null|        null|\n",
      "|196|Check Server Name...|  Started|2021-06-14 18:14:...|         null|        null|\n",
      "|197|Check Server Name...|Succeeded|2021-06-14 18:13:...|         null|        null|\n",
      "|198|Check Server Name...|  Started|2021-06-14 18:13:...|         null|        null|\n",
      "|199|Check Server Name...|Succeeded|2021-06-14 18:13:...|         null|        null|\n",
      "+---+--------------------+---------+--------------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logdf.filter(col(\"Resourcegroup\").isNull()).select(\"Id\",\"Operationname\",\"Status\",\"Time\",\"Resourcegroup\",\"ResourceType\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "32a7fb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+--------------------+--------------------+--------------------+\n",
      "| Id|      Operationname|   Status|                Time|       Resourcegroup|        ResourceType|\n",
      "+---+-------------------+---------+--------------------+--------------------+--------------------+\n",
      "|  1|Delete SQL database|Succeeded|2021-06-15 10:14:...|synapseworkspace-...|Microsoft.Sql/ser...|\n",
      "|  2|Delete SQL database|  Started|2021-06-15 10:14:...|synapseworkspace-...|Microsoft.Sql/ser...|\n",
      "|  3|Delete SQL database| Accepted|2021-06-15 10:14:...|synapseworkspace-...|Microsoft.Sql/ser...|\n",
      "|  4|    Delete SqlPools|Succeeded|2021-06-15 10:14:...|             new-grp|Microsoft.Synapse...|\n",
      "|  5|    Delete SqlPools|  Started|2021-06-15 10:14:...|             new-grp|Microsoft.Synapse...|\n",
      "+---+-------------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logdf.filter(col(\"Resourcegroup\").isNotNull()).select(\"Id\",\"Operationname\",\"Status\",\"Time\",\"Resourcegroup\",\"ResourceType\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "695887d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|           Item Name|Total_count|\n",
      "+--------------------+-----------+\n",
      "|          Pilau Rice|       4721|\n",
      "|          Plain Naan|       3753|\n",
      "|       Plain Papadum|       3598|\n",
      "|         Garlic Naan|       2628|\n",
      "|        Onion Bhajee|       2402|\n",
      "|          Plain Rice|       2369|\n",
      "|Chicken Tikka Masala|       2133|\n",
      "|       Mango Chutney|       2070|\n",
      "|         Bombay Aloo|       1752|\n",
      "|       Peshwari Naan|       1535|\n",
      "|          Mint Sauce|       1463|\n",
      "|       Mushroom Rice|       1452|\n",
      "|          Keema Naan|       1362|\n",
      "|               Korma|       1201|\n",
      "|           Saag Aloo|       1194|\n",
      "|         Meat Samosa|       1192|\n",
      "|             Chapati|       1170|\n",
      "|       Onion Chutney|       1033|\n",
      "|      Butter Chicken|        980|\n",
      "|     Korma - Chicken|        943|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy(col(\"Item Name\")).count().select(col(\"Item Name\"),col(\"count\").alias(\"Total_count\")).sort(col(\"Total_count\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cd090333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "curr = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c16fca95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------+---------+-------------+-----------------------+\n",
      "|ss                       |year(ss)|month(ss)|dayofyear(ss)|to_date(ss, dd/mm/yyyy)|\n",
      "+-------------------------+--------+---------+-------------+-----------------------+\n",
      "|2023-04-16 23:32:52.58307|2023    |4        |106          |2023-04-16             |\n",
      "|2023-04-16 23:32:52.58307|2023    |4        |106          |2023-04-16             |\n",
      "|2023-04-16 23:32:52.58307|2023    |4        |106          |2023-04-16             |\n",
      "+-------------------------+--------+---------+-------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"ss\",lit(curr)).select(col(\"ss\"),year(col(\"ss\")),month(col(\"ss\")),dayofyear(col(\"ss\")),to_date(col(\"ss\"),\"dd/mm/yyyy\")).show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b46c8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
